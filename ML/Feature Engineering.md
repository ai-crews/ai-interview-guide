
# Feature Engineering & Selection
> 작성자: 최고운  
> 키워드: Feature Engineering, Feature Selection

## Feature Engineering

### **개념**
- 머신러닝이나 데이터 분석 모델의 성능을 높이기 위해 기존에 존재하는 변수를 활용하여 새로운 정보를 추가로 생성하는 과정임.
- 이 과정을 통해 컴퓨터가 데이터를 더 잘 이해하고, 예측 능력을 향상시킬 수 있음.
- 예를 들어, 날짜를 연·월·일로 분리하거나 텍스트에서 주제를 추출하고, 수치를 스케일링하거나 로그 변환하는 작업.

### **중요성**
- Garbage in, garbage out: 입력 데이터가 좋지 않으면 아무리 좋은 모델도 좋은 결과를 내기 어려움.
- 데이터에 담긴 도메인 지식을 반영할 수 있음.  
  예: “키”와 “몸무게” 대신 “BMI 지수”로 조합하면 비만 여부 판단이 용이.
- Raw data에는 의미 없는 값이나 불필요한 정보가 섞여 있어 제거 및 가공 필요.

### **주요 기법 및 목적**

| 기법             | 설명                                                       | 예시                                       |
|------------------|----------------------------------------------------------|-------------------------------------------|
| 스케일링         | 데이터 단위를 통일하고 학습 안정성 향상                         | MinMaxScaler, StandardScaler             |
| 인코딩           | 범주형 변수를 수치형으로 변환                                   | 원-핫 인코딩, 타깃 인코딩                  |
| 파생 변수 생성     | 기존 변수로부터 새로운 변수 도출                                 | 날짜→요일, 가격 변화율, 재구매 주기        |
| 텍스트 피처화     | 문자열을 벡터화하여 모델 입력으로 사용                            | CountVectorizer, TF-IDF, Word2Vec        |
| 차원 축소         | 고차원 데이터를 요약해 정보 손실 최소화                           | PCA, UMAP, t-SNE                          |
| 로그 변환         | 큰 수치의 영향을 줄이기 위해 분포를 안정화                         | log(매출 + 1)                              |
| 구간화 (Binning) | 연속형 변수를 일정 범위로 나눔                                   | 나이 → 10대, 20대, 30대                   |

### **Feature Engineering 자동화 도구**

| 도구명                          | 목적                                                    | 주요 활용                           | 특징                                      |
|--------------------------------|-------------------------------------------------------|------------------------------------|-------------------------------------------|
| Featuretools (Python)          | 관계형 데이터 기반 자동 피처 생성                         | 이커머스, 금융                      | 자동 피처 생성 지원                        |
| tsfresh                         | 시계열 데이터의 통계 피처 생성                              | 센서 데이터, 로그 분석               | 다양한 시계열 피처 추출                    |
| Google Vertex AI Feature Store | ML 모델용 피처 중앙 저장 및 관리, 클라우드 기반 공유            | 대규모 모델 운영                     | 실시간/배치 처리 모두 가능                 |
| AWS SageMaker Feature Store     | 학습용 데이터셋에서 피처 추출 후 API로 제공, inference 활용      | 모델 파이프라인, AutoML과 통합 가능 | API 기반으로 실시간 활용 가능               |
| AutoML (H2O.ai, DataRobot 등)   | 피처와 모델을 자동 탐색 및 모델링                             | 빠른 모델링 및 해석                   | 피처 선택 및 모델링을 자동으로 처리 가능     |

---

## Feature Selection

### **개념**
- 데이터에 포함된 여러 변수 중 모델 성능 향상에 도움이 되는 것만 골라내는 과정.

### **중요성**
- 불필요한 피처를 포함하면 모델 성능이 저하될 수 있음 (과적합 위험 증가).
- 모델 학습 속도를 향상시키고 자원 절약 가능.
- 중요한 피처만 남겨 해석력을 높임.

### **주요 기법 및 방식**

#### 1. Filter 방식 (사전 통계 기준 기반)
- 목적: 모델 학습 전, 통계적 기준으로 빠르게 불필요한 변수 제거.

| 기법                     | 작동 원리                                                     | 사용 목적                       | 예시                                 | 장점                             | 단점                             |
|--------------------------|-------------------------------------------------------------|-------------------------------|-------------------------------------|----------------------------------|----------------------------------|
| 분산 임계값 (VarianceThreshold) | 분산이 너무 작은 변수 제거 (대부분 값이 같은 경우 의미 없음)                     | 정보량이 적은 변수 제거              | VarianceThreshold(threshold=0.01) | 빠르고 간단                      | 타겟과 관계 고려 불가             |
| 상관계수 (Correlation)         | feature 간 또는 feature-타겟 간 상관관계 분석, 너무 높거나 낮은 경우 제거          | 중복 변수 제거 또는 관련성 낮은 변수 제거 | df.corr() + 시각화                  | 직관적이고 빠름                   | 비선형 관계 반영 어려움            |
| 카이제곱 테스트 (Chi-Square)    | 범주형 feature와 클래스형 타겟 간 독립성 평가 (p-value 기반 제거)                   | 타겟과 관련 없는 범주형 변수 제거        | SelectKBest(chi2)                  | 범주형 변수에 적합                 | 수치형에는 부적합                  |
| 상호 정보량 (Mutual Information) | feature와 타겟 간 비선형 의존성 측정 (정보량이 적은 변수 제거)                      | 타겟과 관련성 낮은 변수 제거            | mutual_info_classif()               | 비선형 관계 반영 가능              | 직관성이 떨어질 수 있음             |

#### 2. Wrapper 방식 (성능 기준 실험 기반)
- 목적: 모델 성능 중심으로 feature 조합을 최적화.

| 기법                        | 작동 원리                                                      | 사용 목적                          | 예시                                                               | 장점                       |
|-----------------------------|--------------------------------------------------------------|-----------------------------------|-------------------------------------------------------------------|----------------------------|
| SFS (Sequential Forward)    | 빈 피처 집합에서 시작, 하나씩 추가하며 성능 향상되는 feature만 선택                      | 성능 기여도가 높은 feature 단계적 선택 | mlxtend.feature_selection.SequentialFeatureSelector()             | 성능 중심 선택 가능          |
| SBS (Sequential Backward)   | 전체 feature에서 시작, 하나씩 제거하며 성능 유지되는 feature만 남김                         | 중요도가 낮은 feature 제거             | 같은 API (backward=True)                                          | 성능 중심 선택 가능          |
| RFE (Recursive Feature Elimination) | 모델 학습 후 feature 중요도 기반으로 덜 중요한 feature를 반복 제거                       | 모델 기반으로 유익한 subset 선택        | RFE(estimator=...)                                                 | 모델 최적화 가능              |

#### 3. Embedded 방식 (모델 기반 자동 선택)
- 목적: 모델 학습 중 중요도를 계산하여 변수 선택.

| 기법                          | 작동 원리                                                             | 사용 목적                                   | 예시                                                 | 장점                                | 단점                                  |
|-------------------------------|---------------------------------------------------------------------|------------------------------------------|-----------------------------------------------------|-------------------------------------|---------------------------------------|
| Lasso (L1 Regularization)     | 선형 회귀/로지스틱 회귀에서 L1 정규화를 사용하여 중요도 낮은 feature의 계수를 0으로 만듦     | 중요도 낮은 feature 제거 및 모델 간결화                  | Lasso(alpha=0.1)                                      | 해석 쉬움, 자동 feature 제거 가능       | 비선형 관계 반영 어려움, 상관 변수 중 일부만 선택 |
| ElasticNet (L1+L2 Regularization) | Lasso와 Ridge 정규화를 동시에 적용, 계수 축소와 제거를 병행                         | 상관관계 있는 feature 간 균형 유지                       | ElasticNet(alpha=0.1, l1_ratio=0.5)                   | Lasso보다 안정적                      | 하이퍼파라미터 튜닝 필요                   |
| 트리 기반 중요도 (RandomForest, XGBoost 등) | 결정 트리 모델이 Split/Gain 기준으로 feature 중요도를 계산                             | 비선형 문제와 변수 중요도 평가                           | model.feature_importances_                             | 비선형 데이터에 강함, 상호작용 평가 가능         | 연속형 변수가 중요도가 과도하게 높아질 수 있음      |

---

### **예상 질문**
1. Feature Engineering이 무엇인지, 왜 중요한지?
- Feature Engineering은 데이터를 그냥 쓰는 게 아니라 모델이 잘 이해할 수 있도록 가공하거나 새로운 정보를 만들어주는 과정이에요. 이렇게 해야 모델이 더 정확하게 예측할 수 있고 학습도 안정적으로 돼요. 예를 들어 날짜에서 요일을 뽑아내거나 텍스트를 숫자로 변환하는 작업이 있죠.

2. 파생 변수를 생성했던 경험이 있다면 예시와 함께 설명.
- 예전에 고객 구매 데이터를 분석할 때 ‘재구매 주기’나 ‘최근 구매 후 경과 일수’ 같은 새로운 변수를 만든 적이 있어요. 이런 변수를 추가하니 고객 이탈 예측 모델이 훨씬 성능이 좋아졌어요. 단순히 원래 있는 데이터만 쓰는 것보다 의미 있는 변수를 추가하는 게 효과적이라는 걸 느꼈습니다.

3. 텍스트 데이터에서 유용한 피처를 추출하는 방법?
- 기본적으로는 TF-IDF 같은 방법으로 단어를 숫자로 변환해서 쓰고 더 나아가 Word2Vec이나 BERT 같은 모델을 사용해 문장의 의미를 벡터로 뽑을 수 있어요. 상황에 따라 문장 길이나 감성 점수 같은 단순한 값도 함께 쓰면 도움이 됩니다.

4. 수치형 데이터 분포가 치우친 경우 어떻게 처리하고, 왜 그런가요?
- 데이터가 한쪽으로 몰려 있으면 모델이 잘못 학습하거나 특정 값에만 민감해질 수 있어요. 그래서 로그 변환이나 Box-Cox 같은 방법으로 분포를 조금 더 고르게 바꿔줍니다. 이렇게 하면 모델이 데이터를 더 안정적으로 다룰 수 있어요.

5. 로그 변환과 Box-Cox 변환은 각각 언제 쓰나요?
- 로그 변환은 값이 크고 편차가 심한 양수 데이터에 주로 써요. Box-Cox는 로그보다 조금 더 다양한 형태의 분포를 다룰 수 있어서 왜도가 심할 때 유용합니다. 다만 0 이하 값이 있으면 사용이 어렵기 때문에 상황에 따라 골라 씁니다.

6. 범주형 변수를 수치형으로 바꾸는 인코딩 기법과 사용 상황?
- 원-핫 인코딩은 범주 수가 적을 때 간단하게 쓰고 범주가 너무 많으면 타깃 인코딩으로 평균값을 활용해 변환하기도 해요. 순서가 있는 데이터는 라벨 인코딩으로 순서를 반영하는 경우도 있습니다.

7. 이상치는 어떻게 처리하나요?
- 보통 평균에서 너무 벗어나거나 IQR, z-score 같은 기준으로 극단적인 값들을 이상치로 봅니다. 이상치는 그냥 지우기도 하고 값의 범위를 제한하거나 log 변환이나 평균 값으로 바꿔주기도 합니다. 데이터 특성과 모델에 따라 다르게 처리해요.

8. 결측치는 어떻게 처리하나요?
- 결측치는 단순히 평균이나 중앙값으로 채우기도 하고 KNN이나 다른 모델을 써서 예측으로 채우기도 합니다. 경우에 따라서는 해당 컬럼을 아예 제거하거나 결측 여부를 하나의 피처로 만들어 쓰기도 해요. 데이터가 왜 비었는지도 함께 확인해야 합니다.

9. Feature Engineering과 Feature Selection의 차이점?
- Feature Engineering은 새로운 변수를 만들어서 데이터를 풍부하게 만드는 과정이에요. Feature Selection은 그렇게 만들어진 변수 중에서 꼭 필요한 것만 골라서 모델을 단순하게 만드는 과정이고요. 보통 엔지니어링을 먼저 하고 그 다음에 셀렉션을 해요.

10. Feature Selection을 하지 않으면 어떤 문제가 생기나요?
- 필요 없는 변수가 많으면 모델이 노이즈까지 학습해서 과적합이 생기기 쉽습니다. 학습 속도도 느려지고 모델 해석도 어려워져요. 그래서 꼭 필요한 변수만 추리는 과정이 필요합니다.

11. Filter, Wrapper, Embedded 방식의 차이점?
- Filter는 단순히 통계나 상관관계로 변수들을 미리 걸러내는 방식이고 Wrapper는 모델을 계속 돌리면서 성능이 좋은 변수 조합을 찾는 방식이에요. Embedded는 모델이 학습하면서 알아서 중요한 변수를 골라주는 방법입니다.

12. 고차원 데이터에서 과적합이 생기는 이유?
- 변수가 많으면 모델이 데이터의 노이즈까지 외워버리기 쉬워서 새로운 데이터에서는 성능이 안 좋아집니다. 샘플보다 변수가 많으면 이런 문제가 특히 심각해져요. 이걸 막으려면 변수 줄이기나 규제 같은 방법을 씁니다.

13. 상관계수가 높은 변수는 왜 제거하나요?
- 서로 비슷한 정보를 담고 있는 변수가 많으면 모델이 헷갈리거나 회귀 계수가 불안정해집니다. 해석도 어렵고 성능이 떨어질 수 있어요. 그래서 비슷한 변수 중 하나만 남기는 게 좋아요.

14. Lasso와 Ridge의 차이점?
- Lasso는 중요하지 않은 변수의 계수를 0으로 만들어서 변수 자체를 없애는 효과가 있어요. Ridge는 계수를 줄여서 모델을 안정화시키지만 변수를 없애진 않습니다. 변수 선택이 필요하면 Lasso, 안정성이 필요하면 Ridge를 씁니다.

15. ElasticNet이 Lasso보다 나은 상황?
- 변수가 많고 서로 상관관계가 있는 경우 ElasticNet이 L1과 L2를 같이 써서 더 안정적이에요. Lasso는 변수를 과감하게 없애서 정보가 손실될 수 있는데 ElasticNet은 여러 변수를 균형 있게 유지할 수 있습니다.
